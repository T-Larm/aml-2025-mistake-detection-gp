{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573f9594",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab98184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75257417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory - modify to your project path\n",
    "import os\n",
    "PROJECT_PATH = '/content/drive/MyDrive/aml-2025-mistake-detection-gp'  # Change to your path\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install wandb torcheval tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a50dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c28f1b",
   "metadata": {},
   "source": [
    "## 2. Define New LSTM/GRU Models\n",
    "\n",
    "This is the core of Task 2.b: implementing new baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b18b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import existing project modules\n",
    "from core.models.blocks import fetch_input_dim, MLP\n",
    "from constants import Constants as const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based baseline for error recognition.\n",
    "    Task 2.b: New baseline model using bidirectional LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "        input_dimension = fetch_input_dim(config)\n",
    "\n",
    "        # LSTM parameters\n",
    "        self.hidden_size = 512\n",
    "        self.num_layers = 2\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        # LSTM encoder\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dimension,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if self.num_layers > 1 else 0,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # LSTM output dimension\n",
    "        lstm_output_dim = self.hidden_size * 2 if self.bidirectional else self.hidden_size\n",
    "        \n",
    "        # MLP decoder for classification\n",
    "        self.decoder = MLP(lstm_output_dim, 256, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Handle NaN values\n",
    "        input_data = torch.nan_to_num(input_data, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        # Add sequence dimension if needed\n",
    "        if len(input_data.shape) == 2:\n",
    "            input_data = input_data.unsqueeze(1)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(input_data)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states for bidirectional\n",
    "        if self.bidirectional:\n",
    "            forward_hidden = hidden[-2, :, :]\n",
    "            backward_hidden = hidden[-1, :, :]\n",
    "            final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        else:\n",
    "            final_hidden = hidden[-1, :, :]\n",
    "        \n",
    "        # Apply dropout and decode\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "        output = self.decoder(final_hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ErGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based baseline for error recognition.\n",
    "    Task 2.b: Alternative baseline model (lighter than LSTM).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "        input_dimension = fetch_input_dim(config)\n",
    "\n",
    "        # GRU parameters\n",
    "        self.hidden_size = 512\n",
    "        self.num_layers = 2\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        # GRU encoder\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dimension,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if self.num_layers > 1 else 0,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # GRU output dimension\n",
    "        gru_output_dim = self.hidden_size * 2 if self.bidirectional else self.hidden_size\n",
    "        \n",
    "        # MLP decoder for classification\n",
    "        self.decoder = MLP(gru_output_dim, 256, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Handle NaN values\n",
    "        input_data = torch.nan_to_num(input_data, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        # Add sequence dimension if needed\n",
    "        if len(input_data.shape) == 2:\n",
    "            input_data = input_data.unsqueeze(1)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        gru_out, hidden = self.gru(input_data)\n",
    "        \n",
    "        # Concatenate forward and backward hidden states for bidirectional\n",
    "        if self.bidirectional:\n",
    "            forward_hidden = hidden[-2, :, :]\n",
    "            backward_hidden = hidden[-1, :, :]\n",
    "            final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        else:\n",
    "            final_hidden = hidden[-1, :, :]\n",
    "        \n",
    "        # Apply dropout and decode\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "        output = self.decoder(final_hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "print(\"LSTM and GRU models defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e9a69",
   "metadata": {},
   "source": [
    "## 3. Update Model Loading Function\n",
    "\n",
    "Modify `fetch_model` function to support new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5835753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new constants\n",
    "const.LSTM_VARIANT = \"LSTM\"\n",
    "const.GRU_VARIANT = \"GRU\"\n",
    "\n",
    "# Import existing models\n",
    "from core.models.er_former import ErFormer\n",
    "\n",
    "def fetch_model_extended(config):\n",
    "    \"\"\"\n",
    "    Extended model loading function with LSTM and GRU support.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    \n",
    "    if config.variant == const.MLP_VARIANT:\n",
    "        input_dim = fetch_input_dim(config)\n",
    "        model = MLP(input_dim, 512, 1)\n",
    "        \n",
    "    elif config.variant == const.TRANSFORMER_VARIANT:\n",
    "        model = ErFormer(config)\n",
    "        \n",
    "    elif config.variant == const.LSTM_VARIANT:\n",
    "        model = ErLSTM(config)\n",
    "        \n",
    "    elif config.variant == const.GRU_VARIANT:\n",
    "        model = ErGRU(config)\n",
    "\n",
    "    assert model is not None, f\"Model not found: {config.variant}\"\n",
    "    model.to(config.device)\n",
    "    return model\n",
    "\n",
    "print(\"Model loading function extended successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0617c0e",
   "metadata": {},
   "source": [
    "## 4. Configure Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cdbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Experiment configuration dataclass\"\"\"\n",
    "    # Model settings\n",
    "    backbone: str = \"omnivore\"  # omnivore or slowfast\n",
    "    variant: str = \"LSTM\"       # MLP, Transformer, LSTM, GRU\n",
    "    modality: List[str] = None\n",
    "    \n",
    "    # Data settings\n",
    "    split: str = \"recordings\"   # recordings, person, environment\n",
    "    segment_features_directory: str = \"data/\"\n",
    "    \n",
    "    # Training settings\n",
    "    num_epochs: int = 20\n",
    "    batch_size: int = 1\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    \n",
    "    # Other settings\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    task_name: str = \"error_recognition\"\n",
    "    ckpt_directory: str = \"checkpoints/\"\n",
    "    model_name: Optional[str] = None\n",
    "    enable_wandb: bool = False  # Can disable for Colab\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.modality is None:\n",
    "            self.modality = [\"video\"]\n",
    "\n",
    "# Create configuration\n",
    "config = ExperimentConfig()\n",
    "print(f\"\\nExperiment Configuration:\")\n",
    "print(f\"  Model: {config.variant}\")\n",
    "print(f\"  Backbone: {config.backbone}\")\n",
    "print(f\"  Split: {config.split}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090b3b2",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d60f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "from torcheval.metrics.functional import binary_auprc\n",
    "\n",
    "from dataloader.CaptainCookStepDataset import CaptainCookStepDataset, collate_fn\n",
    "\n",
    "\n",
    "def train_and_evaluate(config, model_variant):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model.\n",
    "    \n",
    "    Args:\n",
    "        config: Experiment configuration\n",
    "        model_variant: Model type (MLP, Transformer, LSTM, GRU)\n",
    "    \n",
    "    Returns:\n",
    "        test_metrics: Dictionary of test metrics\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    config.variant = model_variant\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_variant} model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = CaptainCookStepDataset(config, 'train', config.split)\n",
    "    val_dataset = CaptainCookStepDataset(config, 'val', config.split)\n",
    "    test_dataset = CaptainCookStepDataset(config, 'test', config.split)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, \n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, \n",
    "                            collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, \n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    print(f\"Training set: {len(train_dataset)} samples\")\n",
    "    print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "    print(f\"Test set: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Create model\n",
    "    model = fetch_model_extended(config)\n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, \n",
    "                                  weight_decay=config.weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([2.5], dtype=torch.float32).to(config.device)\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, \n",
    "                                   patience=5, verbose=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_auc': []}\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{config.num_epochs}')\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "                \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = evaluate_model(model, val_loader, criterion, config.device)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_auc'].append(val_metrics['auc'])\n",
    "        \n",
    "        scheduler.step(val_metrics['f1'])\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss={val_metrics['loss']:.4f}, \"\n",
    "              f\"Val F1={val_metrics['f1']:.4f}, \"\n",
    "              f\"Val AUC={val_metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save(model.state_dict(), f'best_{model_variant.lower()}_model.pth')\n",
    "    \n",
    "    # Test phase\n",
    "    print(f\"\\nLoading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(f'best_{model_variant.lower()}_model.pth'))\n",
    "    test_metrics = evaluate_model(model, test_loader, criterion, config.device)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{model_variant} Test Results:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return test_metrics, history\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: DataLoader for evaluation\n",
    "        criterion: Loss function\n",
    "        device: Device to use\n",
    "        threshold: Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(output)\n",
    "            preds = (probs >= threshold).float()\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy().flatten())\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy_score(all_targets, all_preds),\n",
    "        'precision': precision_score(all_targets, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_targets, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_targets, all_preds, zero_division=0),\n",
    "        'auc': roc_auc_score(all_targets, all_probs) if len(np.unique(all_targets)) > 1 else 0.5,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Training and evaluation functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fd333",
   "metadata": {},
   "source": [
    "## 6. Run Experiment: Train All Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e97643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all models\n",
    "all_results = {}\n",
    "all_histories = {}\n",
    "\n",
    "# Models to compare\n",
    "models_to_compare = ['MLP', 'Transformer', 'LSTM', 'GRU']\n",
    "\n",
    "# If you only want to test new models, train only LSTM and GRU\n",
    "# models_to_compare = ['LSTM', 'GRU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a083cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "for model_name in models_to_compare:\n",
    "    try:\n",
    "        results, history = train_and_evaluate(config, model_name)\n",
    "        all_results[model_name] = results\n",
    "        all_histories[model_name] = history\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d732c3",
   "metadata": {},
   "source": [
    "## 7. Results Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results comparison table\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "# Convert to percentage display\n",
    "results_pct = results_df.copy()\n",
    "for col in ['accuracy', 'precision', 'recall', 'f1', 'auc']:\n",
    "    if col in results_pct.columns:\n",
    "        results_pct[col] = (results_pct[col] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Model Comparison Results (Test Set, %)\")\n",
    "print(\"=\"*70)\n",
    "print(results_pct.to_string())\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2adfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'loss']\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    if metric in results_df.columns:\n",
    "        values = [results_df.loc[m, metric] if m in results_df.index else 0 \n",
    "                  for m in models_to_compare]\n",
    "        \n",
    "        bars = ax.bar(models_to_compare, values, color=colors[:len(models_to_compare)])\n",
    "        ax.set_title(metric.upper(), fontsize=12, fontweight='bold')\n",
    "        ax.set_ylim([0, 1.1 if metric != 'loss' else max(values) * 1.2])\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.suptitle(f'Baseline Model Comparison ({config.backbone}, {config.split})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChart saved as baseline_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a93938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "for model_name, history in all_histories.items():\n",
    "    ax1.plot(history['train_loss'], label=f'{model_name} (train)', linestyle='-')\n",
    "    ax1.plot(history['val_loss'], label=f'{model_name} (val)', linestyle='--')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# F1 curves\n",
    "ax2 = axes[1]\n",
    "for model_name, history in all_histories.items():\n",
    "    ax2.plot(history['val_f1'], label=model_name)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('Validation F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c2250",
   "metadata": {},
   "source": [
    "## 8. Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print analysis summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Task 2.b Experiment Analysis Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    # Find best models\n",
    "    best_f1_model = max(all_results.items(), key=lambda x: x[1].get('f1', 0))\n",
    "    best_auc_model = max(all_results.items(), key=lambda x: x[1].get('auc', 0))\n",
    "    best_acc_model = max(all_results.items(), key=lambda x: x[1].get('accuracy', 0))\n",
    "    \n",
    "    print(f\"\\nBest Models:\")\n",
    "    print(f\"  - Highest F1 Score: {best_f1_model[0]} ({best_f1_model[1]['f1']*100:.2f}%)\")\n",
    "    print(f\"  - Highest AUC: {best_auc_model[0]} ({best_auc_model[1]['auc']*100:.2f}%)\")\n",
    "    print(f\"  - Highest Accuracy: {best_acc_model[0]} ({best_acc_model[1]['accuracy']*100:.2f}%)\")\n",
    "    \n",
    "    # Compare LSTM/GRU with existing baselines\n",
    "    print(f\"\\nNew Models vs Existing Baselines:\")\n",
    "    \n",
    "    if 'MLP' in all_results and 'LSTM' in all_results:\n",
    "        lstm_vs_mlp = all_results['LSTM']['f1'] - all_results['MLP']['f1']\n",
    "        print(f\"  - LSTM vs MLP (F1): {'+' if lstm_vs_mlp >= 0 else ''}{lstm_vs_mlp*100:.2f}%\")\n",
    "    \n",
    "    if 'Transformer' in all_results and 'LSTM' in all_results:\n",
    "        lstm_vs_trans = all_results['LSTM']['f1'] - all_results['Transformer']['f1']\n",
    "        print(f\"  - LSTM vs Transformer (F1): {'+' if lstm_vs_trans >= 0 else ''}{lstm_vs_trans*100:.2f}%\")\n",
    "    \n",
    "    if 'LSTM' in all_results and 'GRU' in all_results:\n",
    "        gru_vs_lstm = all_results['GRU']['f1'] - all_results['LSTM']['f1']\n",
    "        print(f\"  - GRU vs LSTM (F1): {'+' if gru_vs_lstm >= 0 else ''}{gru_vs_lstm*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('task2b_results.csv')\n",
    "print(\"Results saved to task2b_results.csv\")\n",
    "\n",
    "# Display final results table\n",
    "print(\"\\nFinal Results Table (for report):\")\n",
    "print(results_pct.to_markdown() if hasattr(results_pct, 'to_markdown') else results_pct.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c1737",
   "metadata": {},
   "source": [
    "## 9. Experiment Conclusions\n",
    "\n",
    "Based on the experimental results above, fill in the following conclusions:\n",
    "\n",
    "### Model Performance Comparison\n",
    "| Model | F1 | AUC | Precision | Recall |\n",
    "|-------|----|----|-----------|--------|\n",
    "| MLP (V1) | xx% | xx% | xx% | xx% |\n",
    "| Transformer (V2) | xx% | xx% | xx% | xx% |\n",
    "| **LSTM (New)** | xx% | xx% | xx% | xx% |\n",
    "| **GRU (New)** | xx% | xx% | xx% | xx% |\n",
    "\n",
    "### Key Findings\n",
    "1. LSTM/GRU improvement over MLP: ...\n",
    "2. LSTM/GRU improvement over Transformer: ...\n",
    "3. Effect of sequence modeling on error detection: ...\n",
    "\n",
    "### Recommendations\n",
    "- Recommended model for deployment: ...\n",
    "- Potential improvements: ..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
