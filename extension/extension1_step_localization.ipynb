{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68f781f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/T-Larm/aml-2025-mistake-detection-gp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd aml-2025-mistake-detection-gp\n",
    "!git pull origin main\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9774d",
   "metadata": {},
   "source": [
    "## 2. Path Configuration\n",
    "\n",
    "**‚ö†Ô∏è Modify these paths according to your Google Drive structure!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43affa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ================= PATH CONFIGURATION =================\n",
    "# Modify these paths according to your Google Drive!\n",
    "\n",
    "# Project root (in Colab)\n",
    "PROJECT_ROOT = \"/content/aml-2025-mistake-detection-gp\"\n",
    "\n",
    "# Annotations (from the cloned repo)\n",
    "ANNOTATIONS_PATH = os.path.join(PROJECT_ROOT, \"annotations/complete_step_annotations.json\")\n",
    "\n",
    "# Split file\n",
    "SPLIT_FILE = os.path.join(PROJECT_ROOT, \"er_annotations/recordings_combined_splits.json\")\n",
    "\n",
    "# EgoVLP features on Google Drive\n",
    "# ‚ö†Ô∏è MODIFY THIS PATH according to your Drive structure!\n",
    "EGOVLP_FEATURES_DIR = \"/content/drive/MyDrive/AMLproject/Captain_Cook_dataset/features/segments/egovlp\"\n",
    "\n",
    "# ActionFormer predictions (if using Route B)\n",
    "# Set to None if not using predicted boundaries\n",
    "ACTIONFORMER_PREDICTIONS_PATH = None  # e.g., \"/content/drive/MyDrive/.../actionformer_predictions.json\"\n",
    "\n",
    "# Output directory (save results to Drive for persistence)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/AMLproject/extension_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=== Path Configuration ===\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Annotations: {ANNOTATIONS_PATH}\")\n",
    "print(f\"EgoVLP features: {EGOVLP_FEATURES_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths exist\n",
    "print(\"=== Verifying Paths ===\")\n",
    "\n",
    "# Check annotations\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    print(f\"‚úÖ Annotations file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Annotations file NOT found: {ANNOTATIONS_PATH}\")\n",
    "\n",
    "# Check EgoVLP features\n",
    "if os.path.exists(EGOVLP_FEATURES_DIR):\n",
    "    files = os.listdir(EGOVLP_FEATURES_DIR)\n",
    "    npz_files = [f for f in files if f.endswith('.npz')]\n",
    "    print(f\"‚úÖ EgoVLP features found: {len(npz_files)} .npz files\")\n",
    "    print(f\"   Sample files: {npz_files[:5]}\")\n",
    "else:\n",
    "    print(f\"‚ùå EgoVLP features NOT found: {EGOVLP_FEATURES_DIR}\")\n",
    "\n",
    "# Check split file\n",
    "if os.path.exists(SPLIT_FILE):\n",
    "    print(f\"‚úÖ Split file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Split file NOT found: {SPLIT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e40c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check EgoVLP feature file structure\n",
    "import numpy as np\n",
    "\n",
    "# Find a sample file\n",
    "sample_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) if f.endswith('.npz')][:1]\n",
    "if sample_files:\n",
    "    sample_path = os.path.join(EGOVLP_FEATURES_DIR, sample_files[0])\n",
    "    data = np.load(sample_path)\n",
    "    print(f\"Sample file: {sample_files[0]}\")\n",
    "    print(f\"Keys: {list(data.keys())}\")\n",
    "    for key in data.keys():\n",
    "        print(f\"  {key}: shape = {data[key].shape}, dtype = {data[key].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7684f",
   "metadata": {},
   "source": [
    "## 3. Load Step Localization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from extension.step_localization import (\n",
    "    StepLocalizer,\n",
    "    PredictedBoundaryLocalizer,\n",
    "    prepare_dataset_for_task_verification,\n",
    "    compare_gt_vs_predicted\n",
    ")\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Step localization module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d30328",
   "metadata": {},
   "source": [
    "## 4. Route A: Ground Truth Boundaries\n",
    "\n",
    "This is the **upper bound** baseline. Using perfect step boundaries from annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GT Localizer\n",
    "gt_localizer = StepLocalizer(\n",
    "    annotations_path=ANNOTATIONS_PATH,\n",
    "    features_dir=EGOVLP_FEATURES_DIR,\n",
    "    fps=1.0,  # EgoVLP features are extracted at 1 FPS\n",
    "    feature_key='arr_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10342fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single video\n",
    "# Find a video that has both annotations and features\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Get list of available feature files\n",
    "available_features = set()\n",
    "for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "    if f.endswith('.npz'):\n",
    "        # Extract recording_id from filename: \"9_8_360p_224.mp4_1s_1s.npz\" -> \"9_8\"\n",
    "        recording_id = '_'.join(f.split('_')[:2])\n",
    "        available_features.add(recording_id)\n",
    "\n",
    "# Find videos with both annotations and features\n",
    "annotated_ids = set(annotations.keys())\n",
    "common_ids = annotated_ids.intersection(available_features)\n",
    "print(f\"Videos with both annotations and features: {len(common_ids)}\")\n",
    "print(f\"Sample IDs: {list(common_ids)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single video\n",
    "test_id = list(common_ids)[0]\n",
    "print(f\"\\n=== Processing video: {test_id} ===\")\n",
    "\n",
    "video_data = gt_localizer.process_video(test_id)\n",
    "\n",
    "if video_data:\n",
    "    print(f\"\\nVideo: {video_data.recording_id}\")\n",
    "    print(f\"Activity: {video_data.activity_name}\")\n",
    "    print(f\"Number of steps: {len(video_data.steps)}\")\n",
    "    print(f\"Video label (0=correct, 1=has errors): {video_data.video_label}\")\n",
    "    \n",
    "    print(\"\\nSteps:\")\n",
    "    for i, step in enumerate(video_data.steps):\n",
    "        error_str = \"‚ùå ERROR\" if step.has_errors else \"‚úì\"\n",
    "        print(f\"  [{i+1}] Step {step.step_id}: {step.start_time:.1f}s - {step.end_time:.1f}s {error_str}\")\n",
    "        print(f\"       {step.description[:60]}...\")\n",
    "        print(f\"       Embedding shape: {step.embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b223c5",
   "metadata": {},
   "source": [
    "### 4.1 Process All Available Videos (Route A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all videos that have features\n",
    "print(f\"Processing {len(common_ids)} videos with GT boundaries...\")\n",
    "\n",
    "gt_results = gt_localizer.process_all_videos(list(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ac54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "num_steps_list = [len(v.steps) for v in gt_results.values()]\n",
    "labels = [v.video_label for v in gt_results.values()]\n",
    "\n",
    "print(\"\\n=== Route A Statistics (GT Boundaries) ===\")\n",
    "print(f\"Total videos processed: {len(gt_results)}\")\n",
    "print(f\"Videos with errors: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Videos without errors: {len(labels) - sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "print(f\"Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "print(f\"Min/Max steps: {min(num_steps_list)} / {max(num_steps_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25790c89",
   "metadata": {},
   "source": [
    "### 4.2 Prepare Dataset for Substep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cda628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max steps for padding\n",
    "max_steps = max(len(vd.steps) for vd in gt_results.values())\n",
    "print(f\"Max steps in dataset: {max_steps}\")\n",
    "\n",
    "# Prepare data arrays\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_masks = []\n",
    "all_ids = []\n",
    "\n",
    "for recording_id, video_data in gt_results.items():\n",
    "    embeddings, mask, _ = gt_localizer.get_step_embeddings_matrix(\n",
    "        video_data,\n",
    "        pad_to_length=max_steps\n",
    "    )\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_labels.append(video_data.video_label)\n",
    "    all_masks.append(mask)\n",
    "    all_ids.append(recording_id)\n",
    "\n",
    "# Stack into arrays\n",
    "gt_dataset = {\n",
    "    'embeddings': np.stack(all_embeddings, axis=0),  # (N, max_steps, 256)\n",
    "    'labels': np.array(all_labels),                   # (N,)\n",
    "    'masks': np.stack(all_masks, axis=0),             # (N, max_steps)\n",
    "    'recording_ids': all_ids,\n",
    "    'max_steps': max_steps\n",
    "}\n",
    "\n",
    "print(f\"\\n=== Dataset Ready for Substep 2 ===\")\n",
    "print(f\"Embeddings shape: {gt_dataset['embeddings'].shape}\")\n",
    "print(f\"Labels shape: {gt_dataset['labels'].shape}\")\n",
    "print(f\"Masks shape: {gt_dataset['masks'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc99e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to Google Drive\n",
    "output_path = os.path.join(OUTPUT_DIR, \"gt_step_embeddings.npz\")\n",
    "np.savez(\n",
    "    output_path,\n",
    "    embeddings=gt_dataset['embeddings'],\n",
    "    labels=gt_dataset['labels'],\n",
    "    masks=gt_dataset['masks'],\n",
    "    recording_ids=np.array(gt_dataset['recording_ids'], dtype=object),\n",
    "    max_steps=gt_dataset['max_steps']\n",
    ")\n",
    "print(f\"‚úÖ Dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f4878",
   "metadata": {},
   "source": [
    "## 5. Route B: Predicted Boundaries (ActionFormer)\n",
    "\n",
    "This evaluates the **end-to-end system** using step boundaries predicted by ActionFormer.\n",
    "\n",
    "**‚ö†Ô∏è Skip this section if you don't have ActionFormer predictions yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if you have ActionFormer predictions\n",
    "USE_PREDICTED_BOUNDARIES = False  # Set to True when you have predictions\n",
    "\n",
    "if USE_PREDICTED_BOUNDARIES and ACTIONFORMER_PREDICTIONS_PATH:\n",
    "    pred_localizer = PredictedBoundaryLocalizer(\n",
    "        features_dir=EGOVLP_FEATURES_DIR,\n",
    "        predictions_path=ACTIONFORMER_PREDICTIONS_PATH,\n",
    "        fps=1.0,\n",
    "        confidence_threshold=0.3,  # Filter low confidence predictions\n",
    "        nms_threshold=0.5,          # Remove overlapping predictions\n",
    "        max_predictions=30          # Limit max steps per video\n",
    "    )\n",
    "    print(\"‚úÖ Predicted boundary localizer initialized\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping Route B - No ActionFormer predictions available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If you have predictions in a different format, you can set them manually\n",
    "if USE_PREDICTED_BOUNDARIES:\n",
    "    # Example format for ActionFormer predictions:\n",
    "    # {\n",
    "    #     \"9_8\": [\n",
    "    #         {\"start\": 0, \"end\": 70, \"confidence\": 0.8, \"label\": 1},\n",
    "    #         {\"start\": 72, \"end\": 96, \"confidence\": 0.75, \"label\": 2},\n",
    "    #         ...\n",
    "    #     ]\n",
    "    # }\n",
    "    \n",
    "    # Load your predictions\n",
    "    with open(ACTIONFORMER_PREDICTIONS_PATH, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    pred_localizer.set_predictions(predictions)\n",
    "    \n",
    "    # Process videos\n",
    "    pred_results = pred_localizer.process_all_videos(list(common_ids))\n",
    "    \n",
    "    print(f\"\\n=== Route B Statistics (Predicted Boundaries) ===\")\n",
    "    print(f\"Total videos processed: {len(pred_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61620c0",
   "metadata": {},
   "source": [
    "### 5.1 Compare GT vs Predicted (when available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PREDICTED_BOUNDARIES:\n",
    "    comparison = compare_gt_vs_predicted(\n",
    "        gt_localizer,\n",
    "        pred_localizer,\n",
    "        list(common_ids)\n",
    "    )\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_path = os.path.join(OUTPUT_DIR, \"gt_vs_predicted_comparison.json\")\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump(comparison, f, indent=2)\n",
    "    print(f\"\\n‚úÖ Comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06564690",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "### What we have now:\n",
    "1. **GT Step Embeddings** (`gt_step_embeddings.npz`)\n",
    "   - Shape: `(N, max_steps, 256)`\n",
    "   - Ready for Substep 2 (Task Verification)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Substep 2**: Train a Transformer classifier on step embeddings to predict video-level correctness\n",
    "2. **Substep 3**: Encode task graph nodes with EgoVLP text encoder, match with visual features\n",
    "3. **Substep 4**: Train GNN classifier on the matched task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"Extension Substep 1 Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath) / (1024*1024)  # MB\n",
    "    print(f\"   - {f} ({size:.2f} MB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
