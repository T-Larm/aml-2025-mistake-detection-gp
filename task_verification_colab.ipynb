{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e75a276",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971904fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (if your data is on Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Optional: Change working directory to your project folder\n",
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/aml-2025-mistake-detection-gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc4b11",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository (if not already done)\n",
    "# Uncomment if you need to clone\n",
    "# !git clone https://github.com/your-username/aml-2025-mistake-detection-gp.git\n",
    "# %cd aml-2025-mistake-detection-gp\n",
    "\n",
    "# Or if working from Drive, just cd to the directory\n",
    "# %cd /content/drive/MyDrive/aml-2025-mistake-detection-gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q numpy pandas matplotlib seaborn\n",
    "!pip install -q scikit-learn tqdm\n",
    "\n",
    "print(\"✅ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053967fd",
   "metadata": {},
   "source": [
    "## 3. Import Libraries and Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a532cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "# Make sure you're in the project directory\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from extension.step_localization import StepLocalizer, prepare_dataset_for_task_verification\n",
    "from dataloader.TaskVerificationDataset import TaskVerificationDataset\n",
    "from core.models.task_verifier import TaskVerifier, SimpleMLPVerifier\n",
    "\n",
    "print(\"✅ Project modules imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7918b",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ed565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'features_dir': 'egovlp',\n",
    "    'annotations_file': 'annotations/annotation_json/step_annotations.json',\n",
    "    'split_file': 'er_annotations/recordings_combined_splits.json',\n",
    "    'split': 'train',  # 'train', 'val', or 'test'\n",
    "    \n",
    "    # Model settings\n",
    "    'model_type': 'transformer',  # 'transformer' or 'mlp'\n",
    "    'embedding_dim': 1024,\n",
    "    'hidden_dim': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.3,\n",
    "    \n",
    "    # Training settings\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,  # Early stopping patience\n",
    "    \n",
    "    # Device\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Results\n",
    "    'save_dir': 'results/task_verification',\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae5854",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba97c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize step localizer\n",
    "print(\"Initializing step localizer...\")\n",
    "localizer = StepLocalizer(\n",
    "    annotations_file=CONFIG['annotations_file'],\n",
    "    features_dir=CONFIG['features_dir']\n",
    ")\n",
    "\n",
    "# Prepare dataset\n",
    "print(f\"\\nPreparing dataset from '{CONFIG['split']}' split...\")\n",
    "data_dict = prepare_dataset_for_task_verification(\n",
    "    localizer=localizer,\n",
    "    split_file=CONFIG['split_file'],\n",
    "    split=CONFIG['split']\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Data loaded successfully!\")\n",
    "print(f\"   Shape: {data_dict['embeddings'].shape}\")\n",
    "print(f\"   Samples: {len(data_dict['recording_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataset\n",
    "full_dataset = TaskVerificationDataset(\n",
    "    data_dict['embeddings'],\n",
    "    data_dict['labels'],\n",
    "    data_dict['masks'],\n",
    "    data_dict['recording_ids']\n",
    ")\n",
    "\n",
    "# Print statistics\n",
    "full_dataset.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dda216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "stats = full_dataset.get_statistics()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Class distribution\n",
    "ax = axes[0]\n",
    "labels = ['No Errors', 'Has Errors']\n",
    "counts = [stats['num_negative'], stats['num_positive']]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "ax.bar(labels, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Videos', fontsize=12)\n",
    "ax.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, (label, count) in enumerate(zip(labels, counts)):\n",
    "    ax.text(i, count + 0.5, str(count), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Steps per video distribution\n",
    "ax = axes[1]\n",
    "actual_steps = full_dataset.masks.sum(dim=1).numpy()\n",
    "ax.hist(actual_steps, bins=range(int(actual_steps.min()), int(actual_steps.max())+2), \n",
    "        alpha=0.7, edgecolor='black', color='#3498db')\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Number of Videos', fontsize=12)\n",
    "ax.set_title('Steps per Video Distribution', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.axvline(actual_steps.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {actual_steps.mean():.1f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21170c1f",
   "metadata": {},
   "source": [
    "## 6. Recipe Grouping for Leave-One-Out CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_groups(recording_ids, annotations_file):\n",
    "    \"\"\"Group recordings by recipe (activity_id).\"\"\"\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    recipe_groups = {}\n",
    "    for rec_id in recording_ids:\n",
    "        if rec_id in annotations:\n",
    "            activity_id = annotations[rec_id]['activity_id']\n",
    "            activity_name = annotations[rec_id].get('activity_name', activity_id)\n",
    "            \n",
    "            if activity_id not in recipe_groups:\n",
    "                recipe_groups[activity_id] = {\n",
    "                    'name': activity_name,\n",
    "                    'recordings': []\n",
    "                }\n",
    "            recipe_groups[activity_id]['recordings'].append(rec_id)\n",
    "    \n",
    "    return recipe_groups\n",
    "\n",
    "# Group recordings\n",
    "recipe_groups = get_recipe_groups(data_dict['recording_ids'], CONFIG['annotations_file'])\n",
    "recipe_ids = list(recipe_groups.keys())\n",
    "\n",
    "print(f\"\\nFound {len(recipe_ids)} unique recipes:\")\n",
    "print(\"=\"*60)\n",
    "for recipe_id in recipe_ids:\n",
    "    info = recipe_groups[recipe_id]\n",
    "    print(f\"{recipe_id:15s} - {info['name']:30s} ({len(info['recordings'])} videos)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c99931",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        embeddings = batch['embeddings'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings, masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            embeddings = batch['embeddings'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            outputs = model(embeddings, masks)\n",
    "            probs = outputs.cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            labels_np = labels.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels_np)\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        metrics['auc'] = roc_auc_score(all_labels, all_probs)\n",
    "    else:\n",
    "        metrics['auc'] = 0.0\n",
    "    \n",
    "    return metrics, all_preds, all_labels, all_probs\n",
    "\n",
    "print(\"✅ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e761fad",
   "metadata": {},
   "source": [
    "## 8. Leave-One-Recipe-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "all_fold_results = []\n",
    "fold_predictions = {}  # Store predictions for later analysis\n",
    "\n",
    "device = CONFIG['device']\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Leave-One-Recipe-Out Cross-Validation\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total recipes: {len(recipe_ids)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {CONFIG['model_type']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Loop through each recipe as test set\n",
    "for fold_idx, test_recipe in enumerate(recipe_ids):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold_idx + 1}/{len(recipe_ids)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test recipe: {test_recipe} ({recipe_groups[test_recipe]['name']})\")\n",
    "    \n",
    "    # Split data\n",
    "    test_recordings = recipe_groups[test_recipe]['recordings']\n",
    "    train_recordings = []\n",
    "    for recipe in recipe_ids:\n",
    "        if recipe != test_recipe:\n",
    "            train_recordings.extend(recipe_groups[recipe]['recordings'])\n",
    "    \n",
    "    train_indices = [i for i, rid in enumerate(data_dict['recording_ids']) \n",
    "                    if rid in train_recordings]\n",
    "    test_indices = [i for i, rid in enumerate(data_dict['recording_ids']) \n",
    "                   if rid in test_recordings]\n",
    "    \n",
    "    print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
    "    \n",
    "    if len(test_indices) == 0:\n",
    "        print(\"⚠️  Empty test set, skipping fold\")\n",
    "        continue\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_subset = Subset(full_dataset, train_indices)\n",
    "    test_subset = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    if CONFIG['model_type'] == 'transformer':\n",
    "        model = TaskVerifier(\n",
    "            embedding_dim=CONFIG['embedding_dim'],\n",
    "            hidden_dim=CONFIG['hidden_dim'],\n",
    "            num_heads=CONFIG['num_heads'],\n",
    "            num_layers=CONFIG['num_layers'],\n",
    "            dropout=CONFIG['dropout']\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = SimpleMLPVerifier(\n",
    "            embedding_dim=CONFIG['embedding_dim'],\n",
    "            hidden_dim=CONFIG['hidden_dim'],\n",
    "            dropout=CONFIG['dropout']\n",
    "        ).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                  lr=CONFIG['learning_rate'], \n",
    "                                  weight_decay=CONFIG['weight_decay'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_test_f1 = 0\n",
    "    best_metrics = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    training_history = {'loss': [], 'test_f1': [], 'test_auc': []}\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        training_history['loss'].append(train_loss)\n",
    "        \n",
    "        # Evaluate every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            test_metrics, preds, labels, probs = evaluate(model, test_loader, device)\n",
    "            training_history['test_f1'].append(test_metrics['f1'])\n",
    "            training_history['test_auc'].append(test_metrics['auc'])\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d}/{CONFIG['num_epochs']} - \"\n",
    "                  f\"Loss: {train_loss:.4f} - \"\n",
    "                  f\"F1: {test_metrics['f1']:.4f} - \"\n",
    "                  f\"AUC: {test_metrics['auc']:.4f}\")\n",
    "            \n",
    "            if test_metrics['f1'] > best_test_f1:\n",
    "                best_test_f1 = test_metrics['f1']\n",
    "                best_metrics = test_metrics\n",
    "                best_preds = preds\n",
    "                best_labels = labels\n",
    "                best_probs = probs\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"⚠️  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Store results\n",
    "    if best_metrics is None:\n",
    "        best_metrics, best_preds, best_labels, best_probs = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Fold {fold_idx + 1} Best Results:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    for metric, value in best_metrics.items():\n",
    "        print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    fold_result = {\n",
    "        'fold': fold_idx + 1,\n",
    "        'test_recipe': test_recipe,\n",
    "        'test_recipe_name': recipe_groups[test_recipe]['name'],\n",
    "        'num_train': len(train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'metrics': best_metrics,\n",
    "        'training_history': training_history\n",
    "    }\n",
    "    all_fold_results.append(fold_result)\n",
    "    \n",
    "    fold_predictions[test_recipe] = {\n",
    "        'predictions': best_preds,\n",
    "        'labels': best_labels,\n",
    "        'probabilities': best_probs,\n",
    "        'recording_ids': test_recordings\n",
    "    }\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"✅ Cross-Validation Complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db57bd",
   "metadata": {},
   "source": [
    "## 9. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LEAVE-ONE-RECIPE-OUT CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Completed folds: {len(all_fold_results)}/{len(recipe_ids)}\\n\")\n",
    "\n",
    "avg_metrics = {}\n",
    "for metric in all_fold_results[0]['metrics'].keys():\n",
    "    values = [fold['metrics'][metric] for fold in all_fold_results]\n",
    "    avg_metrics[metric] = {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'values': values\n",
    "    }\n",
    "    \n",
    "    print(f\"{metric.upper():12s}: \"\n",
    "          f\"{avg_metrics[metric]['mean']:.4f} ± {avg_metrics[metric]['std']:.4f} \"\n",
    "          f\"(min: {avg_metrics[metric]['min']:.4f}, max: {avg_metrics[metric]['max']:.4f})\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d69c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results across folds\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Metrics box plot\n",
    "ax = axes[0, 0]\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "data_to_plot = [avg_metrics[m]['values'] for m in metrics_to_plot]\n",
    "bp = ax.boxplot(data_to_plot, labels=[m.upper() for m in metrics_to_plot], \n",
    "                patch_artist=True, showmeans=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#3498db')\n",
    "    patch.set_alpha(0.6)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Metrics Distribution Across Folds', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# 2. Per-recipe F1 scores\n",
    "ax = axes[0, 1]\n",
    "recipe_names = [fold['test_recipe_name'][:20] for fold in all_fold_results]\n",
    "f1_scores = [fold['metrics']['f1'] for fold in all_fold_results]\n",
    "colors_bar = ['#e74c3c' if f1 < 0.5 else '#f39c12' if f1 < 0.7 else '#2ecc71' for f1 in f1_scores]\n",
    "bars = ax.barh(recipe_names, f1_scores, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('F1 Score', fontsize=12)\n",
    "ax.set_title('F1 Score per Recipe', fontsize=14, fontweight='bold')\n",
    "ax.axvline(avg_metrics['f1']['mean'], color='blue', linestyle='--', linewidth=2, label='Mean F1')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Training curves (first fold as example)\n",
    "ax = axes[1, 0]\n",
    "if len(all_fold_results) > 0:\n",
    "    history = all_fold_results[0]['training_history']\n",
    "    epochs_eval = list(range(5, len(history['loss']) + 1, 5))\n",
    "    ax.plot(range(1, len(history['loss']) + 1), history['loss'], \n",
    "            'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12, color='b')\n",
    "    ax.tick_params(axis='y', labelcolor='b')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(epochs_eval, history['test_f1'], 'r-o', linewidth=2, \n",
    "             label='Test F1', markersize=6, alpha=0.7)\n",
    "    ax2.set_ylabel('F1 Score', fontsize=12, color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax.set_title(f\"Training Curve (Fold 1: {all_fold_results[0]['test_recipe_name']})\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "# 4. Confusion matrix (aggregated)\n",
    "ax = axes[1, 1]\n",
    "all_preds_agg = []\n",
    "all_labels_agg = []\n",
    "for recipe, pred_data in fold_predictions.items():\n",
    "    all_preds_agg.extend(pred_data['predictions'])\n",
    "    all_labels_agg.extend(pred_data['labels'])\n",
    "\n",
    "cm = confusion_matrix(all_labels_agg, all_preds_agg)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "            xticklabels=['No Errors', 'Has Errors'],\n",
    "            yticklabels=['No Errors', 'Has Errors'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Aggregated Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results table\n",
    "results_data = []\n",
    "for fold in all_fold_results:\n",
    "    results_data.append({\n",
    "        'Fold': fold['fold'],\n",
    "        'Recipe': fold['test_recipe_name'][:25],\n",
    "        'Train': fold['num_train'],\n",
    "        'Test': fold['num_test'],\n",
    "        'Accuracy': f\"{fold['metrics']['accuracy']:.4f}\",\n",
    "        'Precision': f\"{fold['metrics']['precision']:.4f}\",\n",
    "        'Recall': f\"{fold['metrics']['recall']:.4f}\",\n",
    "        'F1': f\"{fold['metrics']['f1']:.4f}\",\n",
    "        'AUC': f\"{fold['metrics']['auc']:.4f}\",\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(\"\\nDetailed Results per Fold:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "summary_data = []\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:\n",
    "    summary_data.append({\n",
    "        'Metric': metric.upper(),\n",
    "        'Mean': f\"{avg_metrics[metric]['mean']:.4f}\",\n",
    "        'Std': f\"{avg_metrics[metric]['std']:.4f}\",\n",
    "        'Min': f\"{avg_metrics[metric]['min']:.4f}\",\n",
    "        'Max': f\"{avg_metrics[metric]['max']:.4f}\",\n",
    "    })\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a23e7",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ca9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'configuration': CONFIG,\n",
    "    'average_metrics': {k: {key: val for key, val in v.items() if key != 'values'} \n",
    "                       for k, v in avg_metrics.items()},\n",
    "    'fold_results': all_fold_results\n",
    "}\n",
    "\n",
    "results_file = os.path.join(\n",
    "    CONFIG['save_dir'], \n",
    "    f\"loro_cv_{CONFIG['model_type']}_{CONFIG['split']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    ")\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved to: {results_file}\")\n",
    "\n",
    "# Save results table as CSV\n",
    "csv_file = results_file.replace('.json', '.csv')\n",
    "df_results.to_csv(csv_file, index=False)\n",
    "print(f\"✅ Results table saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67a9ce",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Run this cell to see a summary of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK VERIFICATION - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {CONFIG['model_type'].upper()}\")\n",
    "print(f\"Total Recipes: {len(recipe_ids)}\")\n",
    "print(f\"Total Videos: {len(data_dict['recording_ids'])}\")\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  F1 Score:  {avg_metrics['f1']['mean']:.4f} ± {avg_metrics['f1']['std']:.4f}\")\n",
    "print(f\"  AUC Score: {avg_metrics['auc']['mean']:.4f} ± {avg_metrics['auc']['std']:.4f}\")\n",
    "print(f\"  Accuracy:  {avg_metrics['accuracy']['mean']:.4f} ± {avg_metrics['accuracy']['std']:.4f}\")\n",
    "\n",
    "# Identify best and worst recipes\n",
    "best_fold = max(all_fold_results, key=lambda x: x['metrics']['f1'])\n",
    "worst_fold = min(all_fold_results, key=lambda x: x['metrics']['f1'])\n",
    "\n",
    "print(f\"\\nBest Recipe:\")\n",
    "print(f\"  {best_fold['test_recipe_name']} (F1: {best_fold['metrics']['f1']:.4f})\")\n",
    "\n",
    "print(f\"\\nWorst Recipe:\")\n",
    "print(f\"  {worst_fold['test_recipe_name']} (F1: {worst_fold['metrics']['f1']:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✅ Task Verification Training Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e145d3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try different models**: Change `CONFIG['model_type']` to 'mlp' and rerun\n",
    "2. **Hyperparameter tuning**: Adjust learning rate, batch size, hidden dimensions\n",
    "3. **Feature analysis**: Investigate which steps contribute most to predictions\n",
    "4. **Error analysis**: Examine misclassified videos\n",
    "5. **Ensemble methods**: Combine transformer and MLP models\n",
    "6. **Use validation split**: Try training on train+val splits together"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
